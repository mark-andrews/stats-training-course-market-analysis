[
  {
    "objectID": "llm_analysis_howto.html",
    "href": "llm_analysis_howto.html",
    "title": "Text Analysis of ALLSTAT posts using LLMs",
    "section": "",
    "text": "Here, I describe how to use an LLM to extract information from a sample of around 7000 posts to the ALLSTAT mailing list, which is a mailing list about statistics. These posts all contain the term “training” or “course” in their title and so there is a high chance that each one is about a statistics training course. If so, I would like to extract information about the course related to questions like the following:\nIn order to use an LLM to extract answers to these questions, we must use the LLM via an API. These API commands can be run from R, Python, and other languages. Here, I will use R and the ellmer package."
  },
  {
    "objectID": "llm_analysis_howto.html#installation-and-setup",
    "href": "llm_analysis_howto.html#installation-and-setup",
    "title": "Text Analysis of ALLSTAT posts using LLMs",
    "section": "Installation and setup",
    "text": "Installation and setup\nThe ellmer can be install easily from CRAN with install.packages(\"ellmer\"). It will allow you to interface with many LLMs including ChatGPT, Claude, DeepSeek, Llama, and many more. These LLMs may be running locally, i.e. on your own device, or a third-party server.\nUnless the LLM is running locally, you will require a API key. These can be obtained from a developer account. For example, to use ChatGPT via its API in ellmer, you need to first create an OpenAI developer account on the OpenAI developer platform. Once you have done so, go to “API Keys” in your “settings” and then “Create new secret key”. Then, copy the key and save it as as the value of the OPENAI_API_KEY environment variable in your .Renviron in R (see here for more on the .Renviron file).\nWhile it is free to create an OpenAI developer account, there is a fee for using the OpenAI LLMs via the API, see OpenAI API pricing. From what I have seen, prices from other providers are roughly similar.\nAssuming you have an OpenAI API key and you have it saved as the value of the OPENAI_API_KEY environment variable, you can use ChatGPT and other OpenAI LLMs easily in R with ellmer. Here’s a minimal example.\n\nlibrary(ellmer)\n\nclient &lt;- chat_openai(model = 'gpt-4o')\nclient$chat('Tell me a joke about statistics.')\n\n\n\n\nWhy don't statisticians play hide and seek?\n\nBecause good luck hiding — they'll always find you with 95% confidence!\n\n\nIf you have a locally running LLM such as Llama, which can be installed an run using the Ollama software, you can also use it via ellmer. For example, assuming you have Ollama installed, and the Llama3.3 model installed, and the the Ollama server running locally, you can do the following:\n\nclient &lt;- chat_ollama(model = 'llama3.3')\nclient$chat(\"Tell me a joke about statistics\")\n\n\n\n\nWhy the statistician break up with her boyfriend?\n\nBecause their relationship was not significant.\n\n\nAssuming we had the text of a post saved in R as post, if we were trying to extract information concerning those questions listed above and others, we would need to provide detailed instructions about what information to extract. In addition, in order to help with the processing of the extracted information, we would need to provide information about to structure the output that the LLM gives as a responses. This information could be put in a file, such as a markdown formatted file, which could be read in and the provided to the LLM model in R as the value of the system_prompt:\n\nclient &lt;- chat_openai(system_prompt = instructions)\n\nWe could process that post according to the information in instructions as follows:\n\nresult &lt;- client$chat(post)"
  },
  {
    "objectID": "llm_analysis_howto.html#analysing-allstat-posts",
    "href": "llm_analysis_howto.html#analysing-allstat-posts",
    "title": "Text Analysis of ALLSTAT posts using LLMs",
    "section": "Analysing ALLSTAT posts",
    "text": "Analysing ALLSTAT posts\nThe file allstat_training_course_posts.json.bz2 is a compressed json file that contains all the posts to the ALLSTAT mailing list since 1998 that contain the term “training” or “course” in their title.\nThe script llm_analysis.R contains code, including a number of helper functions, to iterate through all of these posts, extract information about the training courses, and post-process and format the results.\nThe exact instructions used to extract the information are in the file instructions.md. In these instructions, we ask the LLM to answer the following questions:\n\nIn one line, what is this course about?\nIn one word or term, what is the course about? For example, “causal inference”, “general linear models”, “Bayesian methods”.\nWhat are the major statistical or data science topics that are covered in the course? For example, generalized linear models, Bayesian data analysis, etc. Here, we are interested in major topics rather minor topics or general and non-specific topics like “data analysis”. Provide this list of major topics list of keywords.\nWhich academic or scientific fields, e.g., biology, economics, psychology, is the course primarily aimed at? Provide a list of academic or research fields as an answer.\nWhat level — beginner, intermediate, advanced — is the course aimed at?\nWhat statistics software package or statistics programming language are used, e.g. Stata, R, Python? We are interested here in just the major statistics software or languages only rather than software that is not specifically for statistics or data analysis.\nIs the course online or in person?\nWhat is the duration of the course? For example, is it a half-day, one-day, two-day, five-day etc course?\nWhich institution or organization is providing the course?\n\nWe also ask the LLM to return the results as a json object in order to facilitate post-processing. An example json object result is as follows:\n\n\n{\n  \"description\": \"Advanced Python programming with a focus on data science for biologists\",\n  \"topic\": \"Python programming\",\n  \"keywords\": [\"data preprocessing\", \"machine learning\", \"data visualization\", \"bioinformatics\"],\n  \"field\": [\"biology\", \"life sciences\"],\n  \"level\": \"intermediate\",\n  \"software\": [\"Python\", \"scikit-learn\", \"matplotlib\", \"seaborn\", \"pytorch\"],\n  \"delivery\": \"online\",\n  \"duration\": \"five-day\",\n  \"provider\": \"Physalia Courses\"\n}\n\n\nThe raw output from the LLM and the post-processed and formatted results are stored in data/allstat_training_course_posts_chatgpt.Rds (ChatGPT based analysis) and data/allstat_training_course_posts_ollama.Rds (Llama 3.3 based analysis)."
  },
  {
    "objectID": "llm_analysis_howto.html#some-results",
    "href": "llm_analysis_howto.html#some-results",
    "title": "Text Analysis of ALLSTAT posts using LLMs",
    "section": "Some results",
    "text": "Some results\nHere, we will take a brief look at some of the results.\nThe data frame containing the results from the LLM is stored in Rds files just mentioned. Here, we read in the data frame of Llama based analysis results.\n\nlibrary(tidyverse)\nlibrary(here)\ntheme_set(theme_classic())\n\nresults_df &lt;- readRDS(here('data/allstat_training_course_posts_ollama.Rds'))$data_frame\nresults_df\n\n# A tibble: 6,843 × 10\n   description                  topic keywords field level software delivery duration provider date \n   &lt;chr&gt;                        &lt;chr&gt; &lt;list&gt;   &lt;lis&gt; &lt;lis&gt; &lt;list&gt;   &lt;list&gt;   &lt;list&gt;   &lt;list&gt;   &lt;chr&gt;\n 1 A hands-on introduction to … Mach… &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    Febr…\n 2 Modelling the Restricted Me… Surv… &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    Febr…\n 3 Introductory medical statis… Medi… &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    Febr…\n 4 Network analysis using R wi… Netw… &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    Febr…\n 5 Introduction to quantitativ… Quan… &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    Febr…\n 6 Statistical modelling using… Stat… &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    Febr…\n 7 Sample size calculations in… Samp… &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    Febr…\n 8 Flow cytometry data analysi… Flow… &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    Febr…\n 9 Introduction to Bayesian st… Baye… &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    Febr…\n10 Python programming for biol… Pyth… &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    Febr…\n# ℹ 6,833 more rows\n\n\nAs you can see, most of the columns are list columns. This is because for many questions, the LLM returns a list of strings rather than a single string. As a result, analysing these results often requires a few more steps than usual. Here is a function that we will use below to counting values in list columns:\n\nlist_col_count &lt;- function(df, col, top_n = 25) {\n  col_name &lt;- rlang::as_string(ensym(col))  # Convert column symbol to string\n  \n  df |&gt; \n    pull({{ col }}) |&gt;  # Extract the list column\n    unlist() |&gt; \n    str_subset(\".\") |&gt;  # Remove empty strings\n    as_tibble(.name_repair = ~ col_name) |&gt;\n    count(.data[[col_name]]) |&gt;\n    arrange(desc(n)) |&gt; \n    slice_head(n = top_n)\n}\n\n\nSoftware\nThe following code looks at which statistics software is used in the courses:\n\nresults_df |&gt; \n  list_col_count(col = software) |&gt; \n  ggplot(aes(x = fct_reorder(software, n), y = n)) + \n    geom_col() +\n    coord_flip() +\n    xlab('Statistics software')\n\n\n\n\n\n\n\n\n\n\nTargetted academic/research fields\nThe following code looks at which academic or research fields the courses are aimed at:\n\nresults_df |&gt; \n  list_col_count(col = field) |&gt; \n  ggplot(aes(x = fct_reorder(field, n), y = n)) + \n    geom_col() +\n    coord_flip() +\n    xlab('Academic/research field')\n\n\n\n\n\n\n\n\n\n\nTopic keywords\nThe following code looks at courses’ topic keywords:\n\nresults_df |&gt; \n  list_col_count(col = keywords) |&gt; \n  ggplot(aes(x = fct_reorder(keywords, n), y = n)) + \n    geom_col() +\n    coord_flip() +\n    xlab('Topic keywords')\n\n\n\n\n\n\n\n\n\n\nProviders\nThe following code looks at which organizations or institutions provide the courses:\n\nresults_df |&gt; \n  list_col_count(col = provider) |&gt; \n  ggplot(aes(x = fct_reorder(provider, n), y = n)) + \n    geom_col() +\n    coord_flip() +\n    xlab('Course provider')"
  }
]